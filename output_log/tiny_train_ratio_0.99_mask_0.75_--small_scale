Fri Aug 16 01:25:47 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090        On  |   00000000:00:08.0 Off |                  Off |
| 30%   38C    P8             19W /  450W |       1MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/cluster/customapps/biomed/vogtlab/abizeul/software/anaconda/envs/reconstruction/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Sum of mask tensor(4718592., device='cuda:0')
Started eval
/cluster/customapps/biomed/vogtlab/abizeul/software/anaconda/envs/reconstruction/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning:

This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.

---- Eval Epoch 10/100
/cluster/customapps/biomed/vogtlab/abizeul/software/anaconda/envs/reconstruction/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:232: UserWarning:

The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.

---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 9/600 in 121.64s with 0.12 loss
---- Epoch 19/600 in 127.09s with 0.11 loss
---- Epoch 29/600 in 123.49s with 0.11 loss
---- Epoch 39/600 in 114.28s with 0.11 loss
---- Epoch 49/600 in 123.29s with 0.11 loss
---- Epoch 59/600 in 114.18s with 0.11 loss
---- Epoch 69/600 in 127.14s with 0.11 loss
---- Epoch 79/600 in 110.46s with 0.11 loss
---- Epoch 89/600 in 130.45s with 0.11 loss
---- Epoch 99/600 in 130.27s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 109/600 in 119.94s with 0.11 loss
---- Epoch 119/600 in 120.98s with 0.11 loss
---- Epoch 129/600 in 133.16s with 0.11 loss
---- Epoch 139/600 in 124.13s with 0.11 loss
---- Epoch 149/600 in 126.53s with 0.11 loss
---- Epoch 159/600 in 123.4s with 0.11 loss
---- Epoch 169/600 in 127.07s with 0.11 loss
---- Epoch 179/600 in 124.21s with 0.11 loss
---- Epoch 189/600 in 117.21s with 0.11 loss
---- Epoch 199/600 in 125.03s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 209/600 in 124.95s with 0.11 loss
---- Epoch 219/600 in 126.18s with 0.11 loss
---- Epoch 229/600 in 122.42s with 0.11 loss
---- Epoch 239/600 in 135.27s with 0.11 loss
---- Epoch 249/600 in 132.67s with 0.11 loss
---- Epoch 259/600 in 128.39s with 0.11 loss
---- Epoch 269/600 in 126.37s with 0.11 loss
---- Epoch 279/600 in 129.69s with 0.11 loss
---- Epoch 289/600 in 126.72s with 0.11 loss
---- Epoch 299/600 in 126.0s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 309/600 in 129.03s with 0.11 loss
---- Epoch 319/600 in 129.42s with 0.11 loss
---- Epoch 329/600 in 136.96s with 0.11 loss
---- Epoch 339/600 in 137.29s with 0.11 loss
---- Epoch 349/600 in 121.37s with 0.11 loss
---- Epoch 359/600 in 137.14s with 0.11 loss
---- Epoch 369/600 in 130.72s with 0.11 loss
---- Epoch 379/600 in 131.03s with 0.11 loss
---- Epoch 389/600 in 120.46s with 0.11 loss
---- Epoch 399/600 in 136.32s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
