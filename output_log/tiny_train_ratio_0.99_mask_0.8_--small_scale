Fri Aug 16 01:26:48 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090        On  |   00000000:00:0A.0 Off |                  Off |
| 30%   37C    P8             28W /  450W |       1MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/cluster/customapps/biomed/vogtlab/abizeul/software/anaconda/envs/reconstruction/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Sum of mask tensor(5111808., device='cuda:0')
Started eval
/cluster/customapps/biomed/vogtlab/abizeul/software/anaconda/envs/reconstruction/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning:

This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.

---- Eval Epoch 10/100
/cluster/customapps/biomed/vogtlab/abizeul/software/anaconda/envs/reconstruction/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:232: UserWarning:

The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.

---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 9/600 in 118.27s with 0.12 loss
---- Epoch 19/600 in 109.37s with 0.12 loss
---- Epoch 29/600 in 112.27s with 0.12 loss
---- Epoch 39/600 in 107.55s with 0.12 loss
---- Epoch 49/600 in 106.47s with 0.12 loss
---- Epoch 59/600 in 114.17s with 0.11 loss
---- Epoch 69/600 in 109.12s with 0.11 loss
---- Epoch 79/600 in 110.65s with 0.11 loss
---- Epoch 89/600 in 116.09s with 0.11 loss
---- Epoch 99/600 in 107.94s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 109/600 in 114.25s with 0.11 loss
---- Epoch 119/600 in 119.39s with 0.11 loss
---- Epoch 129/600 in 110.37s with 0.11 loss
---- Epoch 139/600 in 110.97s with 0.11 loss
---- Epoch 149/600 in 114.31s with 0.11 loss
---- Epoch 159/600 in 113.24s with 0.11 loss
---- Epoch 169/600 in 119.22s with 0.11 loss
---- Epoch 179/600 in 126.81s with 0.11 loss
---- Epoch 189/600 in 125.09s with 0.11 loss
---- Epoch 199/600 in 113.28s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 209/600 in 113.81s with 0.11 loss
---- Epoch 219/600 in 114.85s with 0.11 loss
---- Epoch 229/600 in 122.47s with 0.11 loss
---- Epoch 239/600 in 111.59s with 0.11 loss
---- Epoch 249/600 in 116.94s with 0.11 loss
---- Epoch 259/600 in 118.67s with 0.11 loss
---- Epoch 269/600 in 118.28s with 0.11 loss
---- Epoch 279/600 in 114.43s with 0.11 loss
---- Epoch 289/600 in 121.85s with 0.11 loss
---- Epoch 299/600 in 111.26s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 309/600 in 115.11s with 0.11 loss
---- Epoch 319/600 in 109.86s with 0.11 loss
---- Epoch 329/600 in 113.87s with 0.11 loss
---- Epoch 339/600 in 114.18s with 0.11 loss
---- Epoch 349/600 in 122.81s with 0.11 loss
---- Epoch 359/600 in 118.33s with 0.11 loss
---- Epoch 369/600 in 126.36s with 0.11 loss
---- Epoch 379/600 in 117.57s with 0.11 loss
---- Epoch 389/600 in 122.61s with 0.11 loss
---- Epoch 399/600 in 125.71s with 0.11 loss
Started eval
---- Eval Epoch 10/100
---- Eval Epoch 20/100
---- Eval Epoch 30/100
---- Eval Epoch 40/100
---- Eval Epoch 50/100
---- Eval Epoch 60/100
---- Eval Epoch 70/100
---- Eval Epoch 80/100
---- Eval Epoch 90/100
---- Eval Epoch 100/100
---- Epoch 409/600 in 109.47s with 0.11 loss
---- Epoch 419/600 in 110.87s with 0.11 loss
---- Epoch 429/600 in 114.35s with 0.11 loss
---- Epoch 439/600 in 108.54s with 0.11 loss
